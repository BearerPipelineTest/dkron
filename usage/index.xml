<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Usage on Dkron - Distributed job scheduling system</title>
    <link>/usage/</link>
    <description>Recent content in Usage on Dkron - Distributed job scheduling system</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/usage/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Target nodes spec</title>
      <link>/usage/target-nodes-spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/target-nodes-spec/</guid>
      <description>Target nodes spec You can choose whether a job is run on a node or nodes by specifying tags and a count of target nodes having this tag do you want a job to run.
The target node syntax: `[tag-value]:[count]`  Examples: Target all nodes with a tag:
{ &amp;#34;name&amp;#34;: &amp;#34;job_name&amp;#34;, &amp;#34;command&amp;#34;: &amp;#34;/bin/true&amp;#34;, &amp;#34;schedule&amp;#34;: &amp;#34;@every 2m&amp;#34;, &amp;#34;tags&amp;#34;: { &amp;#34;my_role&amp;#34;: &amp;#34;web&amp;#34; } } mermaid.initialize({startOnLoad:true}); graph LR; J(&#34;Job tags: #quot;my_role#quot;: #quot;web#quot;&#34;) --|Run Job|N1[&#34;</description>
    </item>
    
    <item>
      <title>Cron spec</title>
      <link>/usage/cron-spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/cron-spec/</guid>
      <description>CRON Expression Format A cron expression represents a set of times, using 6 space-separated fields.
Field name | Mandatory? | Allowed values | Allowed special characters ---------- | ---------- | -------------- | -------------------------- Seconds | Yes | 0-59 | * / , - Minutes | Yes | 0-59 | * / , - Hours | Yes | 0-23 | * / , - Day of month | Yes | 1-31 | * / , - ?</description>
    </item>
    
    <item>
      <title>Cloud Auto-join</title>
      <link>/usage/cloud-auto-join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/cloud-auto-join/</guid>
      <description>Cloud Auto-joining As of Dkron 2.0.0, retry-join accepts a unified interface using the go-discover library for doing automatic cluster joining using cloud metadata. To use retry-join with a supported cloud provider, specify the configuration on the command line or configuration file as a key=value key=value ... string.
If the values contain spaces, equals, backslashes or double quotes then they need to be double quoted and the usual escaping rules apply.</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>/usage/clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/clustering/</guid>
      <description>Configure a cluster Dkron can run in HA mode, avoiding SPOFs, this mode provides better scalability and better reliability for users that wants a high level of confidence in the cron jobs they need to run.
Manually bootstrapping a Dkron cluster does not rely on additional tooling, but does require operator participation in the cluster formation process. When bootstrapping, Dkron servers and clients must be started and informed with the address of at least one Dkron server.</description>
    </item>
    
    <item>
      <title>Concurrency</title>
      <link>/usage/concurrency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/concurrency/</guid>
      <description>Concurrency Jobs can be configured to allow overlapping executions or forbid them.
Concurrency property accepts two option:
 allow (default): Allow concurrent job executions. forbid: If the job is already running don&amp;rsquo;t send the execution, it will skip the executions until the next schedule.  Example:
{ &amp;#34;name&amp;#34;: &amp;#34;job1&amp;#34;, &amp;#34;schedule&amp;#34;: &amp;#34;@every 10s&amp;#34;, &amp;#34;executor&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;executor_config&amp;#34;: { &amp;#34;command&amp;#34;: &amp;#34;echo \&amp;#34;Hello from parent\&amp;#34;&amp;#34; }, &amp;#34;concurrency&amp;#34;: &amp;#34;forbid&amp;#34; } </description>
    </item>
    
    <item>
      <title>Embedded storage</title>
      <link>/usage/storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/storage/</guid>
      <description>Dkron has an embedded distributed KV store engine based on BuntDB. This works out of the box on each dkron server.
This ensures a dead easy install and setup, basically run dkron and you will have a full working node.</description>
    </item>
    
    <item>
      <title>Job chaining</title>
      <link>/usage/chaining/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/chaining/</guid>
      <description>Job chaining You can set some jobs to run after other job is executed. To setup a job that will be executed after any other given job, just set the parent_job property when saving the new job.
The dependent job will be executed after the main job finished a successful execution.
Child jobs schedule property will be ignored if it&amp;rsquo;s present.
Take into account that parent jobs must be created before any child job.</description>
    </item>
    
    <item>
      <title>Job metadata</title>
      <link>/usage/metatags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/metatags/</guid>
      <description>Job metadata Jobs can have an optional extra property field called metadata that allows to set arbitrary tags to jobs and query the jobs using the API:
{ &amp;#34;name&amp;#34;: &amp;#34;job_name&amp;#34;, &amp;#34;command&amp;#34;: &amp;#34;/bin/true&amp;#34;, &amp;#34;schedule&amp;#34;: &amp;#34;@every 2m&amp;#34;, &amp;#34;metadata&amp;#34;: { &amp;#34;user_id&amp;#34;: &amp;#34;12345&amp;#34; } } And then query the API to get only the results needed:
$ curl http://localhost:8080/v1/jobs --data-urlencode &amp;quot;metadata[user_id]=12345&amp;quot;` </description>
    </item>
    
    <item>
      <title>Job retries</title>
      <link>/usage/retries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/retries/</guid>
      <description>Jobs can be configured to retry in case of failure.
Configuration { &amp;#34;name&amp;#34;: &amp;#34;job1&amp;#34;, &amp;#34;schedule&amp;#34;: &amp;#34;@every 10s&amp;#34;, &amp;#34;executor&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;executor_config&amp;#34;: { &amp;#34;command&amp;#34;: &amp;#34;echo \&amp;#34;Hello from parent\&amp;#34;&amp;#34; }, &amp;#34;retries&amp;#34;: 5 } In case of failure to run the job in one node, it will try to run the job again in that node until the retries count reaches the limit.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>/usage/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/metrics/</guid>
      <description>Dkron has the ability to send metrics to Statsd for dashboards and historical reporting or provide prometheus format metrics via the api. It sends job processing metrics, golang, and serf metrics.
Configuration Statsd Add this in your yaml config file to enable statsd metrics.
statsd-addr: &amp;#34;localhost:8125&amp;#34; # Or for datadog statsd dog-statsd-addr: &amp;#34;localhost:8125&amp;#34; Prometheus Add this to your yaml config file to enable serving prometheus metrics at the endpoint /metrics</description>
    </item>
    
    <item>
      <title>Outage recovery</title>
      <link>/usage/recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/recovery/</guid>
      <description>Outage Recovery Don&amp;rsquo;t panic! This is a critical first step.
Depending on your deployment configuration, it may take only a single server failure for cluster unavailability. Recovery requires an operator to intervene, but the process is straightforward.
This guide is for recovery from a Dkron outage due to a majority of server nodes in a datacenter being lost. If you are looking to add or remove servers, see the [clustering](/usage/clustering) guide.</description>
    </item>
    
    <item>
      <title>Upgrade methods</title>
      <link>/usage/upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/upgrade/</guid>
      <description>Use one of the following methods (depending on the changes) to upgrade a cluster to a newer version.
Rolling upgrade Use the following procedure to rotate all cluster nodes, one server at a time:
 Add a new servers to the cluster with a configuration that joins them to the existing cluter. Stop dkron service in one of the old servers, if it was the leader allow a new leader to be ellected, note that it is better to remove the current leader at the end, to ensure a leader is elected between the new nodes.</description>
    </item>
    
    <item>
      <title>Use with AWS ECS</title>
      <link>/usage/ecs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/usage/ecs/</guid>
      <description>[Dkron Pro](/products/pro) comes with a [native ECS executor](/pro/ecs) out of the box.  Use with Amazon ECS To use Dkron to schedule jobs that run in containers, a wrapper ECS script is needed.
Install the following snippet in the node that will run the call to ECS
 Prerequisites The node that will run the call to ECS will need to have installed
 AWS cli jq  Example ecs-run --cluster cron --task-definition cron-taskdef --container-name cron --region us-east-1 --command &amp;quot;rake foo&amp;quot;</description>
    </item>
    
  </channel>
</rss>